{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ef51d2-beff-4dc0-a9e4-d72d7a2c4756",
   "metadata": {},
   "source": [
    "## Statistics of Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2363e645-c99d-414d-a6d6-d6a7ec91d6ba",
   "metadata": {},
   "source": [
    "### Subtask B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72e79888-69bf-4e02-9cae-a4a5f505307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122811\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"./data/SubtaskB_split.jsonl\", lines=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f375d26e-ca4e-49db-9b21-f510c82a47de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "wikihow      23556\n",
       "reddit       20999\n",
       "outfox       20999\n",
       "arxiv        20998\n",
       "wikipedia    19368\n",
       "peerread     16891\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1d57209-6c18-4bbe-85a5-ad8a9a579521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "bloomz           17332\n",
       "human            17179\n",
       "chatGPT          16892\n",
       "cohere           16678\n",
       "gpt4             14344\n",
       "davinci          14340\n",
       "dolly            14046\n",
       "gpt-3.5-turbo     6000\n",
       "davinci-003       3000\n",
       "dolly-v2-12b      3000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.model.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728c1c7b-e3ba-4b50-88e5-811698143e7b",
   "metadata": {},
   "source": [
    "### Subtask A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1f0b053-bbeb-42c6-b0b9-cf8a8459d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"./data/SubtaskA.jsonl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54911498-4852-4548-8789-92ce478cb047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['human', 'davinci', 'chatGPT', 'cohere', 'dolly', 'bloomz', 'gpt4']\n",
      "['wikipedia', 'wikihow', 'reddit', 'arxiv', 'peerread']\n"
     ]
    }
   ],
   "source": [
    "# print(df.source.unique(), df.model.unique())\n",
    "models = ['human', 'davinci', 'chatGPT', 'cohere', 'dolly', 'bloomz', 'gpt4']\n",
    "domains = ['wikipedia', 'wikihow', 'reddit', 'arxiv', 'peerread']\n",
    "print(models)\n",
    "print(domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "127c6ad2-982e-4561-b38f-ca3b192b8b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101812\n",
      "\\begin{tabular}{lllllllll}\n",
      "\\toprule\n",
      " & human & davinci & chatGPT & cohere & dolly & bloomz & gpt4 & Total machine \\\\\n",
      "\\midrule\n",
      "wikipedia & 2336 & 3000 & 2995 & 2336 & 2702 & 2999 & 3000 & 14032 \\\\\n",
      "wikihow & 2999 & 3000 & 5557 & 3000 & 3000 & 3000 & 3000 & 17557 \\\\\n",
      "reddit & 3000 & 3000 & 3000 & 3000 & 3000 & 2999 & 3000 & 14999 \\\\\n",
      "arxiv & 2998 & 3000 & 3000 & 3000 & 3000 & 3000 & 3000 & 15000 \\\\\n",
      "peerread & 2847 & 2340 & 2340 & 2342 & 2344 & 2334 & 2344 & 11700 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "50997\n",
      "\\begin{tabular}{lllllllll}\n",
      "\\toprule\n",
      " & human & davinci & chatGPT & cohere & dolly & bloomz & gpt4 & Total machine \\\\\n",
      "\\midrule\n",
      "wikipedia & 11997 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
      "wikihow & 13000 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
      "reddit & 13000 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
      "arxiv & 13000 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
      "peerread & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for label in [True, False]:\n",
    "    data = df[df[\"parallel\"] == label]\n",
    "    print(len(data))\n",
    "    table = {}\n",
    "    for domain in domains:\n",
    "        temp = data[data[\"source\"] == domain]\n",
    "        # print(f\"{domain}: {len(temp)}\")\n",
    "        table[domain] = {}\n",
    "        for model in models:\n",
    "            t = temp[temp[\"model\"] == model]\n",
    "            # print(f\"{model}: {len(t)}\")\n",
    "            table[domain][model] = len(t)\n",
    "        table[domain][\"Total machine\"] = len(temp) - len(temp[temp[\"model\"] == 'human']) - len(temp[temp[\"model\"] == 'gpt4']) \n",
    "    table = pd.DataFrame(table).T\n",
    "    # table\n",
    "    print(table.round(2).astype(str).to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b60ee92e-be9f-479e-9523-96b904c22c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "wikihow      36556\n",
       "reddit       33999\n",
       "arxiv        33998\n",
       "wikipedia    31365\n",
       "peerread     16891\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "313ea104-fde3-4b78-959c-5e76d37b88bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model\n",
       "human      65177\n",
       "chatGPT    16892\n",
       "gpt4       14344\n",
       "davinci    14340\n",
       "bloomz     14332\n",
       "dolly      14046\n",
       "cohere     13678\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.model.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47877773-c534-4719-a78f-f82c0ad6a8d8",
   "metadata": {},
   "source": [
    "## Extract GLTR Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d7eb1229-f764-4ed5-9ff3-b8bb6ff259d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found API <class 'backend.api.LM'> with name gpt-2-small\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"./src/GLTR/\")\n",
    "from src.detector.extract_gltr_features import get_gltr_feature_in_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8153d1cb-b94f-4b9c-bf88-f3fa0f8d7ca5",
   "metadata": {},
   "source": [
    "### Subtask A Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e3de8-441e-4924-83e1-0aecebc0553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('data/SubtaskA_split.jsonl', lines=True)\n",
    "texts = list(data['text'])\n",
    "print(len(texts))\n",
    "\n",
    "features = get_gltr_feature_in_batch(text_list = texts, savepath=\"./data/gltr_features/features_subtaskA.json\")\n",
    "\n",
    "# features = dict(pd.read_json(\"./data/gltr_features/features_subtaskA.json\", typ=\"series\"))\n",
    "assert(len(features) == len(data))\n",
    "data[\"GLTR_features\"] = list(features.values())\n",
    "data.to_json(\"./data/gltr_features/SubtaskA_splits_GLTR.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90140c17-6a07-4faa-a69f-82e42d732ed2",
   "metadata": {},
   "source": [
    "### Subtask B data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd6c88-3cd8-46ee-8a21-0fe5331e788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_json('./data/SubtaskB_split.jsonl', lines=True)\n",
    "texts = list(data['text'])\n",
    "print(len(texts))\n",
    "\n",
    "features = get_gltr_feature_in_batch(text_list = texts, savepath=\"./data/gltr_features/features_subtaskB.json\")\n",
    "# features = dict(pd.read_json(\"./data/gltr_features/features_subtaskB.json\", typ=\"series\"))\n",
    "assert(len(features) == len(data))\n",
    "data[\"GLTR_features\"] = list(features.values())\n",
    "data.to_json(\"./data/gltr_features/SubtaskB_splits_GLTR.jsonl\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957fc1a8-3980-4ce2-9eeb-96a337940271",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb9dd6-9f17-4e7e-8abf-c29e4f2f723b",
   "metadata": {},
   "source": [
    "### Subtask A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf27c7-591f-46e8-9299-80fae666c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.detector.gltr_lr import train, test\n",
    "\n",
    "data = pd.read_json('./data/gltr_features/SubtaskA_splits_GLTR.jsonl', lines=True)\n",
    "# data.loc[9156] \n",
    "data = data.drop(9156) # data[\"GLTR_features\"] = []\n",
    "\n",
    "random_seeds = [0, 10, 30, 55, 75]\n",
    "models = list(data.model.unique())\n",
    "models.remove('human')  # in any settings, we need human data\n",
    "models.append('no')\n",
    "print(models)\n",
    "\n",
    "results_to_save = []\n",
    "for random_seed in random_seeds:\n",
    "    for model in models:\n",
    "        data_train = data[data[f'{model}_{random_seed}'] == 'train']\n",
    "        data_valid = data[data[f'{model}_{random_seed}'] == 'valid']\n",
    "        data_test = data[data[f'{model}_{random_seed}'] == 'test']\n",
    "        # print(len(data_train), len(data_valid), len(data_test))\n",
    "\n",
    "        # get feature and labels\n",
    "        X_train, y_train = get_gltr_features(data_train)\n",
    "        X_valid, y_valid = get_gltr_features(data_valid)\n",
    "        X_test, y_test = get_gltr_features(data_test)\n",
    "        \n",
    "        # train\n",
    "        clf = train(X_train, y_train)\n",
    "\n",
    "        # test\n",
    "        # metrics = eval_binary(clf, X_test, y_test, pos_label=1, average=\"binary\")\n",
    "        results = test(X_test, y_test, clf)\n",
    "        # save results of classification report to 'classification_report_results.jsonl'\n",
    "        res = {\n",
    "            'random_seed': random_seed,\n",
    "            'model': model,\n",
    "            'iteration': clf.n_iter_,\n",
    "            'results': results\n",
    "        }\n",
    "        print(results[\"accuracy\"])\n",
    "        results_to_save.append(res)\n",
    "    pd.Series(results_to_save).to_json(\"test_results/gltr_subtaskA.json\")\n",
    "\n",
    "pd.Series(results_to_save).to_json(\"test_results/gltr_subtaskA.json\")\n",
    "# results_to_save = list(pd.read_json(\"test_results/gltr_subtaskA.json\", typ=\"series\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5dbb4d-c813-41a5-8b2e-f2bb92ab5693",
   "metadata": {},
   "source": [
    "### Subtask B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b667bf-61b3-4617-9df7-514ee2c4fed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arxiv', 'wikipedia', 'wikihow', 'reddit', 'peerread', 'outfox', 'no']\n",
      "Trained [213 225 215 254 214 321 246] iterations.\n",
      "Trained [ 4636 10341 12250  1686 12514 10828  3029  4724  4809  6236  2407  9694\n",
      "  3189  7969  9260  2795 10051  9484  3578  1953  7363] iterations.\n",
      "0.3447471187732165 0.323126012001143\n",
      "Trained [197 246 163 233 190 302 221] iterations.\n",
      "Trained [ 4894 10741 12336  2946 14072 15048  2770  6444  6728  6188  2734  9580\n",
      "  3208  8305  9451  5015 10188  9363  5048  3422  7025] iterations.\n",
      "0.35266174420405844 0.2981360045438116\n",
      "Trained [251 276 203 258 232 278 186] iterations.\n",
      "Trained [ 3710 10712 10994  2511  9848  9354  2949  5513  6154  5911  1672 10071\n",
      "  3585  9316  9137  5395  9658  8050  4044  3187  5650] iterations.\n",
      "0.3877568347767023 0.3597384955000849\n",
      "Trained [247 334 266 218 157 299 184] iterations.\n",
      "Trained [ 5207 11359 12661  2845 11918  9721  3256  6793  6029  6475  3231 10162\n",
      "  3515  8194  9803  5629  9530  8715  4526  3431  5923] iterations.\n",
      "0.46168865184056385 0.46149816657936094\n",
      "Trained [227 234 169 244 163 262 205] iterations.\n",
      "Trained [ 5252 11791 11529  2958 12567  9790  3288  6013  4787  6670  2421  8448\n",
      "  3202  8653  9885  5320 10500  7428  5377  3856  6387] iterations.\n",
      "0.44390503818601623 0.39571369368302645\n",
      "Trained [212 215 167 207 198 245 233] iterations.\n",
      "Trained [ 2897  7726  9733  2409  8996 10949  2180  6383  5972  6241  2833  7741\n",
      "  3544  6567  9035  5697  9747 10111  4932  3532  6748] iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3868755655031192 0.2856802704890709\n",
      "Trained [239 191 190 262 223 321 236] iterations.\n",
      "Trained [ 4378  9202 11261  2513 11850  9115  2634  5596  5669  5533  2458  8680\n",
      "  3048  7472  8949  4885  9550  8103  4259  3084  6348] iterations.\n",
      "0.44966825416208733 0.5035209834330606\n",
      "Trained [287 232 220 246 218 278 197] iterations.\n",
      "Trained [ 4657 10144 12603  1665 11791 10835  2871  5194  4485  6440  2394  9551\n",
      "  3221  7904  9323  2589  9974  9115  3485  2026  7625] iterations.\n",
      "0.3451281074388037 0.32426897799790455\n",
      "Trained [227 221 190 249 178 262 214] iterations.\n",
      "Trained [ 4909 10585 12887  2944 13064 15226  2768  6173  6799  6169  2612 10115\n",
      "  3421  8040  9587  4949 10198  9566  5025  3362  7041] iterations.\n",
      "0.3529715495430371 0.2983941756596272\n",
      "Trained [231 237 188 226 222 255 267] iterations.\n",
      "Trained [ 3749 10620 10748  2794  9620  9539  3029  5573  5367  5928  1643 10448\n",
      "  3798  8850  9074  5662  9500  8014  4076  3233  6017] iterations.\n",
      "0.38550687722873156 0.36147902869757176\n",
      "Trained [247 232 242 274 270 265 191] iterations.\n",
      "Trained [ 5053 11337 12269  2906 11922  9379  3332  6670  6079  6535  2993 11201\n",
      "  3459  8108  9897  5464  9508  8691  4679  3449  6271] iterations.\n",
      "0.46345064050669077 0.4630220486689842\n",
      "Trained [249 265 210 240 200 312 220] iterations.\n",
      "Trained [ 5107 11423 11547  2977 13342  9226  3076  6359  4867  6575  2439  8418\n",
      "  3212  8766  9879  5269 10149  7553  5248  3795  6348] iterations.\n",
      "0.4415369131490143 0.3934639748978746\n",
      "Trained [202 306 224 201 186 239 199] iterations.\n",
      "Trained [ 2814  7637 10224  2658  8805 11045  2188  6592  5796  6428  2770  7548\n",
      "  3607  6911  8923  5634  9545 10493  5111  3792  6942] iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38716129339492356 0.2851088147054622\n",
      "Trained [202 270 189 215 259 281 223] iterations.\n",
      "Trained [ 4139  9434 11344  2548 11864  9593  2683  5757  4925  5444  2553  8992\n",
      "  3124  7647  9087  4691  9188  8224  4190  3189  6138] iterations.\n",
      "0.4508854060655404 0.5060858945654386\n",
      "Trained [245 268 274 222 142 304 211] iterations.\n",
      "Trained [ 4730  9897 12549  1772 11806 10928  3088  4811  4502  6287  2385  9775\n",
      "  3356  7780  9304  2711 10166  8974  3484  1966  7275] iterations.\n",
      "0.3436517763596533 0.32231641108677017\n",
      "Trained [231 311 202 306 228 260 165] iterations.\n",
      "Trained [ 5071 10119 12291  2919 12577 16295  2930  6503  5935  5898  2700  9208\n",
      "  3861  7730  9548  5061 10145  9235  5031  3338  7647] iterations.\n",
      "0.3506996437238602 0.2987039809986059\n",
      "Trained [218 237 162 198 212 295 174] iterations.\n",
      "Trained [ 3627 10045 10528  2434  9636  9473  2931  5534  5542  5758  1709 10470\n",
      "  3975  9085  8947  5398  9299  8147  4125  3205  5467] iterations.\n",
      "0.3867379860757344 0.3620309050772627\n",
      "Trained [218 209 211 260 206 263 262] iterations.\n",
      "Trained [ 5606 11068 14986  3001 11662  9716  3282  6772  6010  6519  2910 10624\n",
      "  3538  8132  9617  5492  9366  8478  4378  3732  6134] iterations.\n",
      "0.4628315634077813 0.4619267584170675\n",
      "Trained [217 207 217 210 210 335 165] iterations.\n",
      "Trained [ 5036 11503 11791  3056 13690  9367  2863  6504  5328  6367  2357  8471\n",
      "  3346  8889 10196  5389 10238  7472  5190  3998  6553] iterations.\n",
      "0.4428985850452904 0.3947072405423006\n",
      "Trained [204 207 189 274 194 232 206] iterations.\n",
      "Trained [ 2714  7140  9473  2407  9035 11053  2157  6245  5641  6266  2589  8085\n",
      "  3393  6467  9160  5639 10006  9886  5093  3505  6648] iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38825658364684035 0.28520405733606363\n",
      "Trained [246 225 227 173 176 309 197] iterations.\n",
      "Trained [ 4210  9486 10821  2576 11389  9873  2685  5876  5124  5879  2396  8791\n",
      "  3247  7139  9021  4665  9096  7976  4312  3221  6115] iterations.\n",
      "0.45052305938861076 0.5013636178613587\n",
      "Trained [217 186 255 270 188 299 229] iterations.\n",
      "Trained [ 4556 10265 13443  1581 12369 10880  2897  4725  4669  6123  2334  9591\n",
      "  3306  7760  9489  2756 10483  8947  3510  1948  7714] iterations.\n",
      "0.3460805791027717 0.32288789408515095\n",
      "Trained [269 219 155 245 226 302 220] iterations.\n",
      "Trained [ 4771 10490 12924  2932 13383 14180  2862  6351  6531  5996  2764  9853\n",
      "  3216  8187  9587  5054 10171  9869  4954  3316  7497] iterations.\n",
      "0.35059637527753396 0.29854907832911654\n",
      "Trained [212 189 168 252 277 214 205] iterations.\n",
      "Trained [ 3584 10848 10881  2490  9853  9465  3003  5974  5523  5798  1656 10449\n",
      "  3556  8550  8926  5409  9423  8109  4159  3262  5637] iterations.\n",
      "0.3879690949227373 0.3633044659534726\n",
      "Trained [260 261 204 197 211 247 220] iterations.\n",
      "Trained [ 5095 11292 12492  2727 11675  9281  3200  6760  5988  6350  3168 10481\n",
      "  3586  7766  9782  5493  9762  8773  4482  3400  5935] iterations.\n",
      "0.46321253393018713 0.4627839420924806\n",
      "Trained [282 261 213 264 219 238 204] iterations.\n",
      "Trained [ 5166 11283 12276  3007 13192  9816  2830  6764  4845  6567  2440  8379\n",
      "  3164  8796  9814  5318 10197  7759  5382  3920  6949] iterations.\n",
      "0.4438458350600912 0.39399680303120005\n",
      "Trained [250 221 205 306 175 312 195] iterations.\n",
      "Trained [ 2797  7222 11734  2493  9166 11424  2197  6494  5834  6216  2661  7955\n",
      "  3645  6404  9443  5920  9791 10382  5149  3565  6825] iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3885423115386447 0.2856802704890709\n",
      "Trained [241 247 189 218 237 273 215] iterations.\n",
      "Trained [ 4290  9497 11121  2645 11890  9272  2776  5755  5448  5636  2602  8974\n",
      "  3218  7476  9067  5114 10169  8304  4184  3162  6329] iterations.\n",
      "0.4487769139973137 0.5029101713541455\n",
      "Trained [259 269 227 204 148 300 222] iterations.\n",
      "Trained [ 4834 10496 12554  1621 12213 10997  2974  4785  4414  6244  2413  9827\n",
      "  3264  8395  9556  2707 10446  9273  3423  1916  7411] iterations.\n",
      "0.3428897990284789 0.3215544337555958\n",
      "Trained [342 235 205 230 213 306 182] iterations.\n",
      "Trained [ 4948 10522 12862  2908 13181 14971  2802  6681  5895  6053  2579  9795\n",
      "  3646  8081  9485  5286 10246  9558  4879  3340  7058] iterations.\n",
      "0.35214540197242733 0.29648370940259205\n",
      "Trained [221 225 199 169 248 258 182] iterations.\n",
      "Trained [ 3852 10599 10716  2420 10033  9300  3076  5847  5676  5634  1780 11062\n",
      "  3885  8895  8883  5621  9314  8024  4233  3269  5531] iterations.\n",
      "0.3888181355068772 0.36258278145695366\n",
      "Trained [203 210 260 247 179 302 193] iterations.\n",
      "Trained [ 5289 11110 13120  2782 12098 10173  3284  6648  5801  6406  2970 10335\n",
      "  3398  7999  9631  5418  9592  8798  4455  3412  6566] iterations.\n",
      "0.46292680603838277 0.4601647697509405\n",
      "Trained [238 292 202 251 305 291 290] iterations.\n",
      "Trained [ 5070 11361 11921  2955 14135  9531  2861  6072  5089  6783  2509  8530\n",
      "  3357  8526  9873  5480 10071  7274  5131  3994  6532] iterations.\n",
      "0.44372742880824106 0.39233911550529865\n",
      "Trained [216 230 244 234 181 231 209] iterations.\n",
      "Trained [ 3025  7358 10314  2389  9119 10938  2185  6630  5874  6036  2637  7982\n",
      "  3690  6705  8890  5642  9812  9940  5096  3570  6724] iterations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/yuxiawang/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.387970855755036 0.28753750178579934\n",
      "Trained [263 256 123 258 189 202 248] iterations.\n",
      "Trained [ 4254  9332 11589  2673 10830  9707  2688  5535  5028  5537  2745  8976\n",
      "  3146  7112  8864  4907  9389  8577  4208  3145  6107] iterations.\n",
      "0.45308747506818087 0.5057597590263362\n"
     ]
    }
   ],
   "source": [
    "# Subtask B is a multiclass classification task, we need to decide classifer we want to use in sklear:\n",
    "# one-vs-one: svc\n",
    "# one-vs-rest: lr\n",
    "import pandas as pd\n",
    "from src.detector.gltr_svm import train, test, get_gltr_features\n",
    "\n",
    "data = pd.read_json('data/gltr_features/SubtaskB_splits_GLTR.jsonl', lines=True)\n",
    "data = data.drop(9156)\n",
    "random_seeds = [0, 10, 30, 55, 75]\n",
    "\n",
    "domains = list(data.source.unique())\n",
    "domains.append('no')\n",
    "print(domains)\n",
    "\n",
    "results_to_save = []\n",
    "for random_seed in random_seeds:\n",
    "    for domain in domains:\n",
    "        data_train = data[data[f'{domain}_{random_seed}'] == 'train']\n",
    "        data_valid = data[data[f'{domain}_{random_seed}'] == 'valid']\n",
    "        data_test = data[data[f'{domain}_{random_seed}'] == 'test']\n",
    "\n",
    "        \n",
    "        # get feature and labels\n",
    "        X_train, y_train = get_gltr_features(data_train)\n",
    "        X_valid, y_valid = get_gltr_features(data_valid)\n",
    "        X_test, y_test = get_gltr_features(data_test)\n",
    "        \n",
    "        # train\n",
    "        clf, svc_dfs = train(X_train, y_train)\n",
    "\n",
    "        # test\n",
    "        results_ovr = test(X_test, y_test, clf)\n",
    "        results_ovo = test(X_test, y_test, svc_dfs)\n",
    "        # save results of classification report to 'classification_report_results.jsonl'\n",
    "        res = {\n",
    "            'random_seed': random_seed,\n",
    "            'domain': domain,\n",
    "            'ovr_iter': clf.n_iter_,\n",
    "            'results_ovr': results_ovr,\n",
    "            'ovo_iter': svc_dfs.n_iter_,\n",
    "            'results_ovo': results_ovo\n",
    "        }\n",
    "        print(results_ovr[\"accuracy\"], results_ovo[\"accuracy\"])\n",
    "        results_to_save.append(res)\n",
    "    pd.Series(results_to_save).to_json(\"test_results/subtaskB_0604.json\")\n",
    "\n",
    "pd.Series(results_to_save).to_json(\"test_results/subtaskB_0604.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f6abac-ef9f-48d4-b57e-20ad8ba1770f",
   "metadata": {},
   "source": [
    "## Print Table results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a427172b-e1c3-46c7-899d-e67ffaee4c4e",
   "metadata": {},
   "source": [
    "### Subtask A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacf89c5-0285-4184-ab0f-1f7d436314b7",
   "metadata": {},
   "source": [
    "- given a model setting, \n",
    "- extract v[\"macro avg\"][\"precision\"], v[\"macro avg\"][\"recall\"], v[\"macro avg\"][\"f1-score\"], v[\"accuracy\"]\n",
    "- calculate the mean and std across five random seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc1af3c-f39e-498e-a1f0-75d33a19853f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_json(\"./test_results/gltr_subtaskA.json\")\n",
    "df = df.T\n",
    "# print(set(list(df[\"model\"])))\n",
    "\n",
    "models = ['no', 'davinci', 'chatGPT', 'gpt4', 'cohere', 'dolly', 'bloomz']\n",
    "random_seeds = [0, 10, 30, 55, 75]\n",
    "\n",
    "# data = df[df[\"model\"]=='no']\n",
    "table = {}\n",
    "for model in models:\n",
    "    data = df[df[\"model\"]==model]\n",
    "\n",
    "    field = [\"1\", \"macro avg\", \"0\"][0]\n",
    "    prec = [v[field][\"precision\"] for v in data[\"results\"]]\n",
    "    recall = [v[field][\"recall\"] for v in data[\"results\"]]\n",
    "    f1 = [v[field][\"f1-score\"] for v in data[\"results\"]]\n",
    "    acc = [v[\"accuracy\"] for v in data[\"results\"]]\n",
    "    metrics = {\"prec\": prec, \"recall\": recall, \"f1-score\": f1, \"accuracy\": acc}\n",
    "    temp = {}\n",
    "    for k, r in metrics.items():\n",
    "        temp[k] = round(np.mean(r)*100,2)\n",
    "    table[model] = temp\n",
    "\n",
    "table = pd.DataFrame(table).T\n",
    "# table\n",
    "print(table.round(2).astype(str).to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd92a0fe-2489-48c6-92a7-53c0114a52f9",
   "metadata": {},
   "source": [
    "### Subtask B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598dd583-15f2-4941-843e-13d069de2d18",
   "metadata": {},
   "source": [
    "- given a domain, and a classifier\n",
    "- extract acc, prec, recall, macro-F1\n",
    "- extract each generator acc\n",
    "- calculate the mean over five seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4a1f061-477f-437a-b30e-2044645153cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllllll}\n",
      "\\toprule\n",
      " & prec & recall & f1-score & accuracy & human & davinci & chatGPT & gpt4 & cohere & dolly & bloomz \\\\\n",
      "\\midrule\n",
      "no & 42.36 & 43.96 & 40.32 & 45.06 & 65.13 & 11.98 & 40.22 & 14.39 & 44.88 & 33.0 & 72.65 \\\\\n",
      "arxiv & 26.24 & 34.45 & 26.92 & 34.45 & 58.38 & 0.06 & 17.78 & 0.34 & 37.59 & 35.84 & 38.47 \\\\\n",
      "peerread & 42.2 & 44.1 & 39.04 & 44.32 & 60.13 & 0.49 & 48.37 & 19.08 & 37.83 & 40.61 & 66.77 \\\\\n",
      "reddit & 45.54 & 46.28 & 41.7 & 46.28 & 85.06 & 24.08 & 22.44 & 19.77 & 55.74 & 14.6 & 70.23 \\\\\n",
      "wikihow & 41.86 & 39.39 & 38.24 & 38.74 & 59.14 & 13.92 & 32.06 & 1.03 & 57.84 & 19.52 & 84.16 \\\\\n",
      "wikipedia & 41.62 & 36.95 & 34.62 & 35.18 & 67.62 & 8.15 & 16.62 & 0.7 & 33.41 & 35.59 & 80.25 \\\\\n",
      "outfox & 29.68 & 32.38 & 29.18 & 38.78 & 49.79 & 12.24 & 37.5 & 0.0 & 1.71 & 17.51 & 85.49 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\\begin{tabular}{llllllllllll}\n",
      "\\toprule\n",
      " & prec & recall & f1-score & accuracy & human & davinci & chatGPT & gpt4 & cohere & dolly & bloomz \\\\\n",
      "\\midrule\n",
      "no & 52.81 & 48.42 & 47.24 & 50.39 & 69.35 & 21.39 & 49.47 & 31.21 & 46.84 & 36.04 & 76.35 \\\\\n",
      "arxiv & 22.57 & 32.29 & 25.73 & 32.28 & 61.34 & 0.0 & 26.36 & 0.0 & 27.79 & 28.63 & 35.99 \\\\\n",
      "peerread & 34.1 & 39.1 & 34.81 & 39.4 & 60.64 & 0.15 & 41.71 & 0.17 & 30.74 & 36.98 & 73.26 \\\\\n",
      "reddit & 43.21 & 46.19 & 40.94 & 46.19 & 84.84 & 0.6 & 38.21 & 9.83 & 53.87 & 19.74 & 79.53 \\\\\n",
      "wikihow & 44.13 & 39.01 & 38.27 & 36.18 & 64.59 & 19.73 & 13.6 & 5.61 & 58.81 & 24.26 & 81.26 \\\\\n",
      "wikipedia & 34.86 & 30.95 & 26.72 & 29.81 & 51.83 & 2.62 & 34.37 & 0.0 & 30.19 & 37.92 & 30.09 \\\\\n",
      "outfox & 26.93 & 28.29 & 26.45 & 28.58 & 51.25 & 27.8 & 2.84 & 0.0 & 0.54 & 15.71 & 86.99 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_json(\"./test_results/subtaskB_0604.json\")\n",
    "# df = pd.read_json(\"./test_results/subtaskB_with_outfox.json\")\n",
    "df = df.T\n",
    "# print(set(list(df['domain'])))\n",
    "# df[df[\"domain\"]==\"no\"]\n",
    "\n",
    "id2label = {0: 'chatGPT', 1: 'human', 2: 'cohere', 3: 'davinci', 4: 'bloomz', 5: 'dolly', 6: 'gpt4'}\n",
    "ids = [1, 3, 0, 6, 2, 5, 4]\n",
    "domains = ['no', 'arxiv', 'peerread', 'reddit', 'wikihow', 'wikipedia', 'outfox']\n",
    "\n",
    "for classifier in [\"results_ovr\", \"results_ovo\"]:\n",
    "    table = {}\n",
    "    for domain in domains:\n",
    "        data = df[df[\"domain\"]==domain]\n",
    "        prec = [v[\"macro avg\"][\"precision\"] for v in data[classifier]]\n",
    "        recall = [v[\"macro avg\"][\"recall\"] for v in data[classifier]]\n",
    "        f1 = [v[\"macro avg\"][\"f1-score\"] for v in data[classifier]]\n",
    "        acc = [v[\"accuracy\"] for v in data[classifier]]\n",
    "        metrics = {\"prec\": prec, \"recall\": recall, \"f1-score\": f1, \"accuracy\": acc}\n",
    "\n",
    "        # each generator f1-score\n",
    "        for generator_index in ids:\n",
    "            metrics[id2label[generator_index]] = [v[str(generator_index)][\"f1-score\"] for v in data[classifier]]\n",
    "        \n",
    "        temp = {}\n",
    "        for k, r in metrics.items():\n",
    "            temp[k] = round(np.mean(r)*100,2)\n",
    "        table[domain] = temp\n",
    "\n",
    "    table = pd.DataFrame(table).T\n",
    "    # table\n",
    "    print(table.round(2).astype(str).to_latex())\n",
    "    table.to_latex(f\"{classifier}_subtaskB.tex\")\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
